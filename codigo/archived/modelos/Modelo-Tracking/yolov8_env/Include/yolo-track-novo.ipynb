{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-image\n",
    "from sort import Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1: Configuração do ambiente e importações\n",
    "\n",
    "# Importações necessárias para todo o notebook\n",
    "import os\n",
    "import sys  # Necessário para modificar o sys.path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO  # Framework para modelos YOLO\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# Adiciona o caminho da pasta 'sort' ao sys.path para que possamos importar o módulo\n",
    "sort_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8\\sort'  # Caminho para a pasta 'sort'\n",
    "sys.path.append(sort_path)  # Adiciona ao sys.path\n",
    "\n",
    "# Verifica se a pasta foi adicionada corretamente\n",
    "print(\"Conteúdo da pasta 'sort':\", os.listdir(sort_path))\n",
    "\n",
    "# Agora podemos importar o módulo 'Sort' corretamente\n",
    "try:\n",
    "    from sort import Sort  # Algoritmo de tracking SORT (importado do repositório original)\n",
    "    print(\"Módulo 'Sort' importado com sucesso!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Erro ao importar o módulo 'Sort': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2: Definição dos caminhos dos dados e configuração\n",
    "# Caminho base do dataset (alterado conforme meu diretório local)\n",
    "dataset_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8'\n",
    "train_path = os.path.join(dataset_path, 'train', 'images')\n",
    "val_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "test_path = os.path.join(dataset_path, 'test', 'images')\n",
    "\n",
    "# Exibindo algumas informações sobre os dados\n",
    "print(\"Caminho do dataset:\", dataset_path)\n",
    "print(\"Imagens de treino:\", len(os.listdir(train_path)))\n",
    "print(\"Imagens de validação:\", len(os.listdir(val_path)))\n",
    "print(\"Imagens de teste:\", len(os.listdir(test_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3: Configuração e treinamento do modelo YOLOv8\n",
    "# Carrega um modelo YOLOv8 pré-treinado para utilizar como base\n",
    "model = YOLO('yolov8n.pt')  # Usando a versão \"nano\" para treinar mais rápido\n",
    "\n",
    "# Treinamento do modelo no meu dataset customizado por 15 épocas\n",
    "model.train(\n",
    "    data=os.path.join(dataset_path, 'data.yaml'),  # Arquivo YAML com as classes e caminhos\n",
    "    epochs=15,  # Definindo 15 épocas para acelerar o processo\n",
    "    imgsz=640,  # Tamanho das imagens para treinamento\n",
    "    batch=8,  # Tamanho do lote, ajustado para evitar problemas de memória\n",
    "    name='YOLOv8_Better_Beef_Training',  # Nome do projeto de treinamento\n",
    "    save=True  # Salvando checkpoints durante o treinamento\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4: Carregamento do modelo treinado para inferência\n",
    "model_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8\\yolov8_env\\Include\\runs\\detect\\YOLOv8_Better_Beef_Training\\weights\\best.pt'\n",
    "\n",
    "# Caminho do vídeo de entrada (alterado para meu diretório local)\n",
    "video_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8\\imagem_termica1.mp4'\n",
    "\n",
    "# Definindo intervalo para cortar do meio do vídeo (1.5 minutos, do meio)\n",
    "start_time = 1200  # Início do corte em segundos (20 minutos)\n",
    "end_time = start_time + 90  # Final do corte (21.5 minutos)\n",
    "output_video_path = 'output_segment.mp4'  # Nome do arquivo de saída com o corte\n",
    "\n",
    "# Função para cortar o vídeo no intervalo desejado\n",
    "def cut_video(input_path, output_path, start, end):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)  # Obtém o FPS do vídeo original\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    current_frame = 0  # Inicializa contagem de frames\n",
    "    start_frame = int(start * fps)  # Frame inicial do corte\n",
    "    end_frame = int(end * fps)  # Frame final do corte\n",
    "    \n",
    "    # Loop para percorrer o vídeo e realizar o corte\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or current_frame > end_frame:  # Interrompe se atingir o final do corte\n",
    "            break\n",
    "        if current_frame >= start_frame:  # Escreve os frames dentro do intervalo\n",
    "            out.write(frame)\n",
    "        current_frame += 1\n",
    "    \n",
    "    # Libera os recursos de vídeo\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Segmento de vídeo salvo em: {output_path}\")  # Informa a conclusão do corte\n",
    "\n",
    "# Chamando a função para cortar o vídeo\n",
    "cut_video(video_path, output_video_path, start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5: Tracking dos animais no vídeo cortado\n",
    "# Caminho de entrada e saída para o vídeo com tracking\n",
    "input_video_path = output_video_path  # Vídeo de entrada é o segmento cortado\n",
    "output_tracked_video = 'tracked_video.mp4'  # Nome do vídeo de saída com tracking\n",
    "\n",
    "# Inicializa a captura do vídeo e parâmetros de escrita\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # FPS do vídeo cortado\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_tracked_video, fourcc, fps, (width, height))\n",
    "\n",
    "# Inicializa o algoritmo de tracking SORT para manter o ID dos animais\n",
    "tracker = Sort()\n",
    "\n",
    "# Variável para contagem de animais (IDs únicos)\n",
    "animal_count = 0\n",
    "\n",
    "# Loop para processar cada frame do vídeo\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:  # Interrompe quando acaba o vídeo\n",
    "        break\n",
    "    \n",
    "    # Realiza a inferência no frame usando o modelo treinado\n",
    "    results = model(frame)\n",
    "    \n",
    "    # Extrai bounding boxes e scores das detecções\n",
    "    detections = []\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            if len(box.xyxy) > 0:  # Verifica se há detecções\n",
    "                x1, y1, x2, y2 = box.xyxy.cpu().numpy().astype(int)[0]  # Coordenadas da bounding box\n",
    "                conf = box.conf.cpu().numpy()[0]  # Confiança da detecção\n",
    "                detections.append([x1, y1, x2, y2, conf])  # Adiciona ao array de detecções\n",
    "    \n",
    "    # Converte para numpy array e verifica se há detecções\n",
    "    detections = np.array(detections)\n",
    "    if len(detections) > 0:  # Se houver detecções, faz o tracking\n",
    "        tracked_objects = tracker.update(detections)\n",
    "        \n",
    "        # Desenha bounding boxes e IDs nos frames\n",
    "        for obj in tracked_objects:\n",
    "            x1, y1, x2, y2, obj_id = obj.astype(int)  # Convertendo para int\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Desenha a caixa\n",
    "            cv2.putText(frame, f\"ID {int(obj_id)}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "        \n",
    "        # Atualiza a contagem de animais com base nos IDs únicos\n",
    "        animal_count = len(set(tracked_objects[:, 4]))\n",
    "    \n",
    "    # Escreve o frame com tracking no vídeo de saída\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Mostra o frame processado (útil para visualizar durante a execução)\n",
    "    cv2.imshow('Frame com Tracking', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Para visualização rápida, usa 'q' para sair\n",
    "        break\n",
    "\n",
    "# Libera os recursos de vídeo\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(f\"Vídeo processado e salvo em: {output_tracked_video}\")\n",
    "print(f\"Total de animais rastreados: {animal_count}\")  # Exibe contagem final de animais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6: Exibição e análise final do tracking e contagem\n",
    "# Função para exibir o vídeo processado no notebook\n",
    "def show_video(video_path, width=640, height=480):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # Interrompe quando acaba o vídeo\n",
    "            break\n",
    "        frame = cv2.resize(frame, (width, height))  # Redimensiona para visualização\n",
    "        display(Image(data=cv2.imencode('.jpg', frame)[1].tobytes()))  # Mostra frame\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Exibir o vídeo processado com tracking\n",
    "show_video(output_tracked_video)\n",
    "\n",
    "# Contagem total de animais rastreados no vídeo\n",
    "print(f\"Contagem total de animais rastreados: {animal_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from sort import Sort\n",
    "\n",
    "# Configuração do modelo e parâmetros\n",
    "model_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8\\yolov8_env\\Include\\runs\\detect\\YOLOv8_Better_Beef_Training\\weights\\best.pt'\n",
    "model = YOLO(model_path)  # Carrega o modelo treinado\n",
    "\n",
    "# Caminhos de vídeo\n",
    "input_video_path = r'C:\\Users\\wasse\\Documentos\\Modelo-YOLO-V8\\imagem_termica1.mp4'\n",
    "output_video_path = 'tracked_video_segment.mp4'\n",
    "\n",
    "# Inicializa a captura do vídeo\n",
    "cap = cv2.VideoCapture(input_video_path)\n",
    "fps = 10  # Define um novo FPS menor para processamento (10 quadros por segundo)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Inicializa o tracker SORT\n",
    "tracker = Sort()\n",
    "\n",
    "# Parâmetros de detecção\n",
    "conf_threshold = 0.5  # Confiança mínima para considerar uma detecção válida\n",
    "frame_count = 0  # Contador de frames processados\n",
    "\n",
    "# Contadores para cabeças e olhos\n",
    "total_cabecas_detectadas = 0\n",
    "total_olhos_detectados = 0\n",
    "\n",
    "# Classes de detecção (supondo que as classes estão definidas no modelo YOLO)\n",
    "classe_cabeca = 0  # Supondo que a classe '0' representa cabeça\n",
    "classe_olho = 1  # Supondo que a classe '1' representa olho\n",
    "\n",
    "# Posição inicial e final do segmento a ser processado (em segundos)\n",
    "start_time = 4 * 60  # Minuto 4\n",
    "end_time = 6 * 60  # Minuto 6\n",
    "\n",
    "# Definir o ponto inicial do vídeo em frames\n",
    "cap.set(cv2.CAP_PROP_POS_MSEC, start_time * 1000)  # Define o ponto inicial em milissegundos\n",
    "\n",
    "# Loop para processar cada frame dentro do segmento desejado\n",
    "while cap.isOpened():\n",
    "    current_time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000  # Tempo atual em segundos\n",
    "    if current_time > end_time:  # Interrompe se atingir o final do segmento\n",
    "        break\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:  # Interrompe se não houver mais frames\n",
    "        break\n",
    "    \n",
    "    # Processa apenas a cada 3 frames para reduzir a carga\n",
    "    if frame_count % 3 == 0:  # Modifique para alterar a frequência de processamento\n",
    "        # Realiza a inferência no frame\n",
    "        results = model(frame)\n",
    "        \n",
    "        # Coleta as detecções\n",
    "        detections = []\n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy.cpu().numpy().astype(int)[0]\n",
    "                conf = box.conf.cpu().numpy()[0]\n",
    "                cls = int(box.cls.cpu().numpy()[0])  # Classe da detecção\n",
    "                \n",
    "                # Filtra as detecções com base na confiança mínima\n",
    "                if conf >= conf_threshold:\n",
    "                    detections.append([x1, y1, x2, y2, conf])\n",
    "                    \n",
    "                    # Incrementa o contador de acordo com a classe detectada\n",
    "                    if cls == classe_cabeca:\n",
    "                        total_cabecas_detectadas += 1\n",
    "                    elif cls == classe_olho:\n",
    "                        total_olhos_detectados += 1\n",
    "        \n",
    "        # Converte para numpy array e atualiza o tracker\n",
    "        detections = np.array(detections)\n",
    "        if len(detections) > 0:\n",
    "            tracked_objects = tracker.update(detections)\n",
    "            \n",
    "            # Desenha as caixas e IDs rastreados\n",
    "            for obj in tracked_objects:\n",
    "                x1, y1, x2, y2, obj_id = obj.astype(int)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"ID {int(obj_id)}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    # Escreve o frame processado no vídeo de saída\n",
    "    out.write(frame)\n",
    "    \n",
    "    # Mostra o frame processado\n",
    "    cv2.imshow('Frame com Tracking', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Pressione 'q' para sair\n",
    "        break\n",
    "    \n",
    "    frame_count += 1  # Incrementa o contador de frames\n",
    "\n",
    "# Libera os recursos de vídeo\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Exibe as contagens finais de detecções\n",
    "print(f\"Vídeo processado e salvo em: {output_video_path}\")\n",
    "print(f\"Total de cabeças detectadas: {total_cabecas_detectadas}\")\n",
    "print(f\"Total de olhos detectados: {total_olhos_detectados}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
